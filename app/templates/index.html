<!doctype html>
<html class="h-full bg-gray-900 text-white font-sans">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>SPOC-Shot Demo</title>
  <script src="https://unpkg.com/htmx.org@1.9.10"></script>
  <script src="https://cdn.tailwindcss.com"></script>
  <script type="module">
    import { CreateMLCEngine } from "https://esm.run/@mlc-ai/web-llm";
    window.CreateMLCEngine = CreateMLCEngine;
  </script>
  <style>
    /* Custom styles for the toggle switch */
    .toggle-checkbox:checked {
      right: 0;
      border-color: #4A5568;
    }
    .toggle-checkbox:checked + .toggle-label {
      background-color: #4A5568;
    }
    /* Simple fade-in animation for log entries */
    @keyframes fadeIn {
      from { opacity: 0; transform: translateY(10px); }
      to { opacity: 1; transform: translateY(0); }
    }
    .log-entry {
      animation: fadeIn 0.3s ease-out;
    }
  </style>
</head>
<body class="h-full flex flex-col items-center justify-center p-4 sm:p-6 lg:p-8">

  <div class="w-full max-w-7xl mx-auto">
    <header class="text-center mb-6">
      <h1 class="text-4xl sm:text-5xl font-bold text-white">SPOC-Shot</h1>
      <p class="text-gray-400 mt-2">A Live Demo of Single-Pass Self-Correcting Agent Loops</p>
    </header>

    <!-- Model Loading Overlay (initially hidden) -->
    <div id="model-loading-panel" class="fixed inset-0 bg-black/50 flex items-center justify-center z-50" style="display: none;">
      <div class="bg-gray-800 border border-blue-700 p-8 rounded-lg shadow-2xl max-w-md w-full mx-4">
        <h2 class="text-xl font-semibold mb-4 flex items-center text-center">
          <span class="mr-2">🧠</span> Initializing WebLLM
        </h2>
        <div class="mb-6">
          <div class="flex justify-between text-sm text-gray-300 mb-2">
            <span id="model-status">Starting WebLLM initialization...</span>
            <span id="model-progress-text">0%</span>
          </div>
          <div class="w-full bg-gray-700 rounded-full h-3">
            <div id="model-progress-bar" class="bg-blue-600 h-3 rounded-full transition-all duration-300" style="width: 0%"></div>
          </div>
        </div>
        <p class="text-sm text-gray-400 text-center">
          Loading Qwen2.5-0.5B-Instruct (~500MB)<br>
          <span class="text-xs text-gray-500">This happens once and caches locally</span>
        </p>
      </div>
    </div>

    <div class="grid grid-cols-1 lg:grid-cols-2 gap-6">
      <!-- Left Column -->
      <div class="flex flex-col gap-6">
        <!-- Controls Panel -->
        <div class="bg-gray-800 p-6 rounded-lg shadow-2xl">
          <h2 class="text-xl font-semibold mb-4">Controls</h2>
          <form id="prompt-form"
                hx-post="/solve"
                hx-trigger="submit"
                hx-ext="sse"
                hx-swap="none">
            <div class="mb-4">
              <label for="agent-mode" class="block text-sm font-medium text-gray-300 mb-2">Agent Mode</label>
              <div class="relative inline-block w-full text-sm">
                  <select id="agent-mode" name="mode" class="w-full bg-gray-700 border border-gray-600 rounded-md p-3 focus:outline-none focus:ring-2 focus:ring-blue-500">
                      <option value="multi_pass" selected>Multi-Pass (Baseline)</option>
                      <option value="single_pass">Single-Pass (SPOC)</option>
                  </select>
              </div>
            </div>
            <div class="mb-4">
              <label for="prompt-input" class="block text-sm font-medium text-gray-300 mb-2">Demo Prompt</label>
              <input id="prompt-input" name="prompt" class="w-full bg-gray-700 border border-gray-600 rounded-md p-3 focus:outline-none focus:ring-2 focus:ring-blue-500" value="How many conversions did we get this week?">
            </div>
            <button id="run-button" type="submit" class="w-full bg-blue-600 hover:bg-blue-700 text-white font-bold py-3 px-4 rounded-md transition-colors disabled:bg-gray-500">
              Run Agent
            </button>
          </form>
        </div>

        <!-- Metrics Panel -->
        <div class="bg-gray-800 p-6 rounded-lg shadow-2xl">
          <h2 class="text-xl font-semibold mb-4">Real-Time Performance</h2>
          <div class="grid grid-cols-3 gap-4 text-center">
            <div>
              <p class="text-gray-400 text-sm">Latency</p>
              <p id="metric-latency" class="text-3xl font-bold">0.00s</p>
            </div>
            <div>
              <p class="text-gray-400 text-sm">Input Tokens</p>
              <p id="metric-prompt-tokens" class="text-3xl font-bold">0</p>
            </div>
            <div>
              <p class="text-gray-400 text-sm">Output Tokens</p>
              <p id="metric-completion-tokens" class="text-3xl font-bold">0</p>
            </div>
            <div>
              <p class="text-gray-400 text-sm">LLM Calls</p>
              <p id="metric-llm-calls" class="text-3xl font-bold">0</p>
            </div>
          </div>
        </div>
      </div>

      <!-- Right Column -->
      <div class="flex flex-col gap-6">
        <!-- Code View Panel -->
        <div class="bg-gray-800 p-6 rounded-lg shadow-2xl">
          <h2 class="text-xl font-semibold mb-4">Agent Logic</h2>
          <div id="code-view" class="bg-gray-900 p-4 rounded-md text-xs font-mono">
            <!-- Pseudo-code will be injected here -->
          </div>
        </div>
      </div>
    </div>

    <!-- Log Stream Panel -->
    <div class="mt-6 bg-gray-800 p-6 rounded-lg shadow-2xl">
      <h2 class="text-xl font-semibold mb-4">Log Stream</h2>
      <div id="log-container" class="h-96 overflow-y-auto bg-gray-900 p-4 rounded-md">
        <div id="log" class="space-y-2 text-sm font-mono">
          <!-- Log entries will be appended here -->
        </div>
      </div>
    </div>
  </div>

  <!-- Pseudo-code templates -->
  <template id="multi-pass-code">
    <pre><code class="language-python">
# Multi-Pass (ReAct)
for attempt in max_retries:
  # 1. First LLM Call
  tool_call = llm.think(prompt)

  # 2. Execute Tool
  result = execute(tool_call)

  if result.is_success():
    # 3. Second LLM Call
    answer = llm.summarize(result)
    return answer
  else:
    # 4. Loop to Retry
    prompt = f"Fix this: {result.error}"
    </code></pre>
  </template>

  <template id="single-pass-code">
    <pre><code class="language-python">
# Single-Pass (SPOC)
while retries < max_retries:
  # 1. Single, Stateful LLM Call
  response = llm.refine(prompt)

  if response.has_tool_call():
    # 2. Execute Tool
    result = execute(response.tool_call)
    # 3. Feed result back into the SAME call
    prompt = f"Result: {result}"
  else:
    # 4. Get final answer
    return response.answer
    </code></pre>
  </template>

  <script>
    document.addEventListener('DOMContentLoaded', () => {
      const form = document.getElementById('prompt-form');
      const log = document.getElementById('log');
      const runButton = document.getElementById('run-button');
      const modeSelect = document.getElementById('agent-mode');
      const codeView = document.getElementById('code-view');
      const multiPassTemplate = document.getElementById('multi-pass-code');
      const singlePassTemplate = document.getElementById('single-pass-code');

      const latencyEl = document.getElementById('metric-latency');
      const promptTokensEl = document.getElementById('metric-prompt-tokens');
      const completionTokensEl = document.getElementById('metric-completion-tokens');
      const llmCallsEl = document.getElementById('metric-llm-calls');

      // WebLLM elements
      const modelLoadingPanel = document.getElementById('model-loading-panel');
      const modelStatus = document.getElementById('model-status');
      const modelProgressText = document.getElementById('model-progress-text');
      const modelProgressBar = document.getElementById('model-progress-bar');

      // --- State Management ---
      let currentMode = 'multi_pass';
      let webllmEngine = null;
      let modelLoaded = false;

      const updateCodeView = () => {
        codeView.innerHTML = currentMode === 'single_pass'
          ? singlePassTemplate.innerHTML
          : multiPassTemplate.innerHTML;
      };

      const resetMetrics = () => {
        latencyEl.textContent = '0.00s';
        promptTokensEl.textContent = '0';
        completionTokensEl.textContent = '0';
        llmCallsEl.textContent = '0';
      };

      const updateMetrics = (metrics) => {
        if (!metrics) return;
        latencyEl.textContent = `${(metrics.latency || 0).toFixed(2)}s`;
        promptTokensEl.textContent = metrics.prompt_tokens || 0;
        completionTokensEl.textContent = metrics.completion_tokens || 0;
        llmCallsEl.textContent = metrics.llm_calls || 0;
      };

      // --- WebLLM Initialization ---
      const initializeWebLLM = async () => {
        try {
          modelLoadingPanel.style.display = 'block';
          runButton.disabled = true;
          
          const selectedModel = "Qwen2.5-0.5B-Instruct-q4f32_1";
          
          modelStatus.textContent = "Checking WebGPU support...";
          
          // Check WebGPU support
          if (!navigator.gpu) {
            throw new Error("WebGPU not supported in this browser");
          }
          
          // Test WebGPU adapter
          const adapter = await navigator.gpu.requestAdapter();
          if (!adapter) {
            throw new Error("No WebGPU adapter found");
          }

          modelStatus.textContent = "Initializing WebLLM engine...";
          
          const initProgressCallback = (report) => {
            const progress = Math.round(report.progress * 100);
            modelProgressText.textContent = `${progress}%`;
            modelProgressBar.style.width = `${progress}%`;
            modelStatus.textContent = report.text || "Loading model...";
          };

          webllmEngine = await window.CreateMLCEngine(
            selectedModel,
            { initProgressCallback }
          );

          modelStatus.textContent = "Model loaded successfully!";
          modelProgressText.textContent = "100%";
          modelProgressBar.style.width = "100%";
          
          setTimeout(() => {
            modelLoadingPanel.style.display = 'none';
            runButton.disabled = false;
            modelLoaded = true;
          }, 1000);

        } catch (error) {
          console.error("WebLLM initialization failed:", error);
          modelStatus.textContent = `Error: ${error.message}`;
          modelProgressBar.style.backgroundColor = '#dc2626';
          
          // Show helpful error message without popup
          setTimeout(() => {
            // Update the overlay to show error
            const panel = modelLoadingPanel.querySelector('div');
            panel.innerHTML = `
              <div class="text-center">
                <div class="text-red-400 text-xl mb-4">⚠️ WebLLM Unavailable</div>
                <div class="text-sm text-gray-300 mb-6 text-left">
                  <strong>Most likely cause:</strong> WebGPU not enabled in Chrome<br><br>
                  <strong>Quick fix:</strong><br>
                  1. Go to <code class="bg-gray-700 px-1 rounded">chrome://flags</code><br>
                  2. Search for "webgpu"<br>
                  3. Enable "#enable-unsafe-webgpu"<br>
                  4. Restart Chrome<br><br>
                  <strong>Alternative:</strong> Try Firefox (often enabled by default)
                </div>
                <button onclick="location.reload()" class="bg-blue-600 hover:bg-blue-700 text-white px-4 py-2 rounded mr-3">
                  Retry
                </button>
                <button onclick="enableDemoMode()" class="bg-gray-600 hover:bg-gray-700 text-white px-4 py-2 rounded">
                  View Demo UI
                </button>
              </div>
            `;
            
            // Also update the main UI
            runButton.disabled = true;
            runButton.textContent = 'WebLLM Required';
            runButton.className = runButton.className.replace('bg-blue-600', 'bg-gray-600');
          }, 1000);
        }
      };

      // --- Event Listeners ---
      modeSelect.addEventListener('change', (e) => {
        currentMode = e.target.value;
        updateCodeView();
      });

      form.addEventListener('htmx:beforeRequest', (e) => {
        log.innerHTML = '<div class="text-gray-500">Waiting for agent...</div>';
        runButton.disabled = true;
        runButton.textContent = 'Running...';
        resetMetrics();

        // HTMX doesn't have a clean way to send form data as JSON
        // when also using the SSE extension. We will cancel the HTMX
        // request and send our own fetch request.
        e.preventDefault();

        const prompt = document.getElementById('prompt-input').value;
        
        // Check if we should use WebLLM or server
        if (modelLoaded && webllmEngine) {
          // Use WebLLM for inference
          runWebLLMAgent(prompt, currentMode);
        } else if (runButton.textContent.includes('Demo Mode')) {
          // Demo mode - show simulated error
          log.innerHTML = '<div class="text-yellow-500">Demo Mode: WebLLM is required for actual inference</div>';
          runButton.disabled = false;
          runButton.textContent = 'Demo Mode - Click to See Error';
          return;
        } else {
          // Fallback to server-based inference
          const payload = {
              prompt: prompt,
              mode: currentMode
          };
          
          console.log("Using server mode. Sending payload:", JSON.stringify(payload, null, 2));

          // Use the browser's native fetch API to stream the response
          fetch('/solve', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify(payload)
          })
        .then(response => {
            const reader = response.body.getReader();
            const decoder = new TextDecoder();

            // Clear the "Waiting" message
            if (log.firstElementChild && log.firstElementChild.textContent.includes('Waiting')) {
                log.innerHTML = '';
            }

            function push() {
                reader.read().then(({ done, value }) => {
                    if (done) {
                        runButton.disabled = false;
                        runButton.textContent = 'Run Agent';
                        return;
                    }
                    // The response is a stream of "data: {...}" events.
                    // We need to parse them.
                    const chunk = decoder.decode(value);
                    const lines = chunk.split('\n').filter(line => line.trim().startsWith('data:'));
                    for (const line of lines) {
                        const jsonStr = line.replace('data:', '').trim();
                        if (jsonStr) {
                            const data = JSON.parse(jsonStr);
                            handleSseMessage(data);
                        }
                    }
                    push();
                });
            }
            push();
        })
        .catch(err => {
            console.error('Fetch error:', err);
            runButton.disabled = false;
            runButton.textContent = 'Run Agent';
        });
        }
      });

      // --- WebLLM Agent Implementation ---
      const runWebLLMAgent = async (prompt, mode) => {
        console.log(`Running WebLLM agent in ${mode} mode with prompt: ${prompt}`);
        
        // Clear log and reset UI
        log.innerHTML = '<div class="text-gray-500">Running agent with WebLLM...</div>';
        resetMetrics();
        
        const startTime = performance.now();
        let metrics = { prompt_tokens: 0, completion_tokens: 0, latency: 0, llm_calls: 0 };
        
        try {
          if (mode === 'multi_pass') {
            await runMultiPassWebLLM(prompt, metrics, startTime);
          } else {
            await runSinglePassWebLLM(prompt, metrics, startTime);
          }
        } catch (error) {
          console.error('WebLLM agent error:', error);
          handleSseMessage({
            phase: 'error',
            message: `WebLLM error: ${error.message}`,
            metrics: metrics
          });
        } finally {
          runButton.disabled = false;
          runButton.textContent = 'Run Agent';
        }
      };

      const runMultiPassWebLLM = async (prompt, metrics, startTime) => {
        const requestId = `spoc-shot-${Date.now()}`;
        
        // Simulate the multi-pass logic
        for (let attempt = 0; attempt < 3; attempt++) {
          handleSseMessage({ phase: 'propose', metrics });
          
          // First LLM call
          const systemPrompt = `You are an agent designed for a specific demo. Your ONLY purpose is to answer the user's question about "conversions".
1. You MUST use the sql_query tool. It is the only tool available.
2. The sql_query tool takes a single argument: column.
3. The user's question is "How many conversions did we get this week?". You should infer the correct column name from this.
4. Your first action MUST be to call the tool with what you think the column name is. Output a TOOL_CALL in JSON format.
5. After you receive the EXEC_RESULT, if it failed, analyze the 'hint' and call the tool again with the corrected column name.
6. If the result is successful, provide a one-sentence answer summarizing the data.
Example of a tool call:
TOOL_CALL: {"name": "sql_query", "args": {"column": "your_best_guess"}}`;

          const messages = [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: prompt }
          ];

          const completion = await webllmEngine.chat.completions.create({
            messages,
            temperature: 0.0,
            // Note: WebLLM might handle request_id differently
          });

          metrics.llm_calls += 1;
          metrics.prompt_tokens += completion.usage?.prompt_tokens || 0;
          metrics.completion_tokens += completion.usage?.completion_tokens || 0;
          
          const response = completion.choices[0].message.content;
          handleSseMessage({ phase: 'model_response', content: response, metrics });
          
          // Check for tool call
          if (response.includes('TOOL_CALL:')) {
            const toolCallStr = response.split('TOOL_CALL:')[1].trim();
            try {
              const toolCall = JSON.parse(toolCallStr);
              handleSseMessage({ phase: 'execute', call: toolCall, metrics });
              
              // Execute tool (client-side simulation)
              const result = simulateToolCall(toolCall);
              handleSseMessage({ phase: 'tool_result', result, metrics });
              
              if (result.ok) {
                // Success - make final LLM call for answer
                messages.push({ role: 'assistant', content: response });
                messages.push({ role: 'tool', content: JSON.stringify(result) });
                
                handleSseMessage({ phase: 'propose', metrics });
                const finalCompletion = await webllmEngine.chat.completions.create({
                  messages,
                  temperature: 0.0,
                });
                
                // Don't increment llm_calls for same request_id (simulating KV cache)
                metrics.prompt_tokens += finalCompletion.usage?.prompt_tokens || 0;
                metrics.completion_tokens += finalCompletion.usage?.completion_tokens || 0;
                
                const finalAnswer = finalCompletion.choices[0].message.content;
                metrics.latency = (performance.now() - startTime) / 1000;
                handleSseMessage({ phase: 'success', answer: finalAnswer, metrics });
                return;
              } else {
                // Tool failed, continue loop
                messages.push({ role: 'assistant', content: response });
                messages.push({ role: 'tool', content: JSON.stringify(result) });
                handleSseMessage({ phase: 'failure', message: 'Tool execution failed. Retrying...', metrics });
              }
            } catch (e) {
              handleSseMessage({ phase: 'failure', message: `Invalid tool call: ${e.message}`, metrics });
              break;
            }
          } else {
            metrics.latency = (performance.now() - startTime) / 1000;
            handleSseMessage({ phase: 'success', answer: response, metrics });
            return;
          }
        }
        
        handleSseMessage({ phase: 'error', message: 'Agent failed after multiple attempts', metrics });
      };

      const runSinglePassWebLLM = async (prompt, metrics, startTime) => {
        const requestId = `spoc-shot-${Date.now()}`;
        
        // Single-pass logic
        for (let attempt = 0; attempt < 3; attempt++) {
          handleSseMessage({ phase: 'propose', metrics });
          
          const systemPrompt = `You are an agent designed for a specific demo. Your ONLY purpose is to answer the user's question about "conversions".
1. You will be given a TOOL_SIGNATURE for the sql_query tool. It is the only tool you can use.
2. The tool takes a single argument: column.
3. The user's question is "How many conversions did we get this week?". Infer the column name from this question.
4. Your first action MUST be to call the tool. Output a TOOL_CALL in JSON format.
5. If the EXEC_RESULT you receive is a failure, you MUST use the 'hint' to immediately try a new TOOL_CALL with the corrected column name.
6. If the EXEC_RESULT is successful, provide a one-sentence answer summarizing the data.
Example of a tool call:
TOOL_CALL: {"name": "sql_query", "args": {"column": "conversions"}}`;

          const toolSignature = `TOOL_SIGNATURE: {
  "name": "sql_query",
  "description": "Query the company database.",
  "parameters": {
    "type": "object",
    "properties": {
      "column": {
        "type": "string",
        "description": "The column to query, e.g., 'users', 'revenue', 'convs'."
      }
    },
    "required": ["column"]
  }
}`;

          const messages = [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: `${toolSignature}\n\nUser Prompt: ${prompt}` }
          ];

          const completion = await webllmEngine.chat.completions.create({
            messages,
            temperature: 0.0,
          });

          metrics.llm_calls += 1;
          metrics.prompt_tokens += completion.usage?.prompt_tokens || 0;
          metrics.completion_tokens += completion.usage?.completion_tokens || 0;
          
          const response = completion.choices[0].message.content;
          handleSseMessage({ phase: 'model_response', content: response, metrics });
          
          // Check for tool call
          if (response.includes('TOOL_CALL:')) {
            const toolCallStr = response.split('TOOL_CALL:')[1].trim();
            try {
              const toolCall = JSON.parse(toolCallStr);
              handleSseMessage({ phase: 'execute', call: toolCall, metrics });
              
              // Execute tool
              const result = simulateToolCall(toolCall);
              handleSseMessage({ phase: 'tool_result', result, metrics });
              
              if (result.ok) {
                // Success - generate final answer (simulating continued generation)
                messages.push({ role: 'assistant', content: response });
                messages.push({ role: 'tool', content: JSON.stringify(result) });
                
                const finalCompletion = await webllmEngine.chat.completions.create({
                  messages,
                  temperature: 0.0,
                });
                
                // In true single-pass, this would be continuation, not new call
                metrics.prompt_tokens += finalCompletion.usage?.prompt_tokens || 0;
                metrics.completion_tokens += finalCompletion.usage?.completion_tokens || 0;
                
                const finalAnswer = finalCompletion.choices[0].message.content;
                metrics.latency = (performance.now() - startTime) / 1000;
                handleSseMessage({ phase: 'success', answer: finalAnswer, metrics });
                return;
              } else {
                // Tool failed - show patch phase and continue
                handleSseMessage({ phase: 'patch', message: 'Tool execution failed. Attempting to self-patch.', metrics });
                messages.push({ role: 'assistant', content: response });
                messages.push({ role: 'tool', content: JSON.stringify(result) });
              }
            } catch (e) {
              handleSseMessage({ phase: 'failure', message: `Invalid tool call: ${e.message}`, metrics });
              break;
            }
          } else {
            metrics.latency = (performance.now() - startTime) / 1000;
            handleSseMessage({ phase: 'success', answer: response, metrics });
            return;
          }
        }
        
        handleSseMessage({ phase: 'error', message: 'Agent failed to self-patch after multiple attempts', metrics });
      };

      // Simulate the tool execution client-side
      const simulateToolCall = (toolCall) => {
        const column = toolCall.args?.column || toolCall.arguments?.column;
        
        if (column === "conversions") {
          return { ok: false, hint: "Did you mean 'convs'?" };
        } else if (column === "convs") {
          return { ok: true, data: 12345 };
        } else {
          return { ok: false, hint: `Column '${column}' not found.` };
        }
      };

      function handleSseMessage(data) {
        const entry = document.createElement('div');
        entry.className = 'log-entry p-2 rounded-md flex items-start';

        const now = new Date();
        const timestamp = `[${now.getHours().toString().padStart(2, '0')}:${now.getMinutes().toString().padStart(2, '0')}:${now.getSeconds().toString().padStart(2, '0')}]`;

        const ICONS = {
          propose: '🧠',
          execute: '⚙️',
          failure: '❌',
          patch: '🔧',
          success: '✅',
          tool_result: '📦',
          model_response: '💬',
          error: '🔥'
        };

        let icon = ICONS[data.phase] || '➡️';
        let content = '';
        let colorClass = 'bg-gray-700/50';

        switch (data.phase) {
          case 'propose':
            content = `<span class="font-bold text-cyan-400">PROPOSE</span>: Agent is thinking...`;
            break;
          case 'model_response':
            content = `<span class="font-bold text-gray-400">MODEL</span>: <pre class="inline-block whitespace-pre-wrap">${data.content}</pre>`;
            break;
          case 'execute':
            content = `<span class="font-bold text-purple-400">EXECUTE</span>: Calling tool <code class="text-white">${data.call.name}</code> with args <code class="text-white">${JSON.stringify(data.call.args)}</code>`;
            break;
          case 'tool_result':
            const resultClass = data.result.ok ? 'text-green-400' : 'text-red-400';
            content = `<span class="font-bold ${resultClass}">RESULT</span>: <pre class="inline-block whitespace-pre-wrap">${JSON.stringify(data.result)}</pre>`;
            break;
          case 'failure':
             content = `<span class="font-bold text-red-500">FAILURE</span>: ${data.message}`;
             colorClass = 'bg-red-900/30';
             break;
          case 'patch':
            content = `<span class="font-bold text-yellow-400">PATCH</span>: Tool failed. Attempting to self-correct...`;
            colorClass = 'bg-yellow-900/30';
            break;
          case 'success':
            content = `<span class="font-bold text-green-400">SUCCESS</span>: <pre class="inline-block whitespace-pre-wrap">${data.answer}</pre>`;
            colorClass = 'bg-green-900/30';
            break;
          case 'error':
            content = `<span class="font-bold text-red-500">ERROR</span>: ${data.message}`;
            colorClass = 'bg-red-900/50 border border-red-700';
            break;
        }
        
        entry.className += ` ${colorClass}`;
        entry.innerHTML = `<span class="mr-2 text-gray-500">${timestamp}</span><span class="mr-3">${icon}</span> <div class="flex-1">${content}</div>`;
        log.appendChild(entry);
        log.scrollTop = log.scrollHeight;

        updateMetrics(data.metrics);
      }

      // Remove the now-unused global SSE listener
      // document.body.removeEventListener('htmx:sseMessage', ...);

      form.addEventListener('htmx:afterRequest', () => {
        runButton.disabled = false;
        runButton.textContent = 'Run Agent';
      });

      document.body.addEventListener('htmx:sseMessage', (event) => {
        if (log.firstElementChild && log.firstElementChild.textContent.includes('Waiting')) {
            log.innerHTML = ''; // Clear "Waiting" message
        }
        
        const data = JSON.parse(event.detail.data);
        const entry = document.createElement('div');
        entry.className = 'log-entry p-2 rounded-md flex items-start';

        const ICONS = {
          propose: '🧠',
          execute: '⚙️',
          failure: '❌',
          patch: '🔧',
          success: '✅',
          tool_result: '📦',
          model_response: '💬',
          error: '🔥'
        };

        let icon = ICONS[data.phase] || '➡️';
        let content = '';
        let colorClass = 'bg-gray-700/50';

        switch (data.phase) {
          case 'propose':
            content = `<span class="font-bold text-cyan-400">PROPOSE</span>: Agent is thinking...`;
            break;
          case 'model_response':
            content = `<span class="font-bold text-gray-400">MODEL</span>: <pre class="inline-block whitespace-pre-wrap">${data.content}</pre>`;
            break;
          case 'execute':
            content = `<span class="font-bold text-purple-400">EXECUTE</span>: Calling tool <code class="text-white">${data.call.name}</code> with args <code class="text-white">${JSON.stringify(data.call.args)}</code>`;
            break;
          case 'tool_result':
            const resultClass = data.result.ok ? 'text-green-400' : 'text-red-400';
            content = `<span class="font-bold ${resultClass}">RESULT</span>: <pre class="inline-block whitespace-pre-wrap">${JSON.stringify(data.result)}</pre>`;
            break;
          case 'failure':
             content = `<span class="font-bold text-red-500">FAILURE</span>: ${data.message}`;
             colorClass = 'bg-red-900/30';
             break;
          case 'patch':
            content = `<span class="font-bold text-yellow-400">PATCH</span>: Tool failed. Attempting to self-correct...`;
            colorClass = 'bg-yellow-900/30';
            break;
          case 'success':
            content = `<span class="font-bold text-green-400">SUCCESS</span>: <pre class="inline-block whitespace-pre-wrap">${data.answer}</pre>`;
            colorClass = 'bg-green-900/30';
            break;
          case 'error':
            content = `<span class="font-bold text-red-500">ERROR</span>: ${data.message}`;
            colorClass = 'bg-red-900/50 border border-red-700';
            break;
        }
        
        entry.className += ` ${colorClass}`;
        entry.innerHTML = `<span class="mr-3">${icon}</span> <div class="flex-1">${content}</div>`;
        log.appendChild(entry);
        log.scrollTop = log.scrollHeight;

        updateMetrics(data.metrics);
      });

      // --- App Initialization ---
      const initializeApp = async () => {
        try {
          // Check server configuration
          const configResponse = await fetch('/api/config');
          const config = await configResponse.json();
          
          console.log('Server config:', config);
          
          if (config.server_available) {
            // Server mode available - initialize WebLLM as enhancement
            console.log('Server available, initializing WebLLM as enhancement...');
            initializeWebLLM();
          } else {
            // WebLLM required mode
            console.log('Server not available, WebLLM required...');
            modelStatus.textContent = "Server not available. WebLLM required for inference.";
            initializeWebLLM();
          }
        } catch (error) {
          console.error('Failed to check server config:', error);
          // Assume WebLLM required
          initializeWebLLM();
        }
      };

      // --- Demo Mode (when WebLLM fails) ---
      window.enableDemoMode = () => {
        modelLoadingPanel.style.display = 'none';
        
        // Show demo notice
        const demoNotice = document.createElement('div');
        demoNotice.className = 'mb-6 bg-yellow-900/30 border border-yellow-700 p-4 rounded-lg text-center';
        demoNotice.innerHTML = `
          <div class="text-yellow-400 font-semibold mb-2">📖 Demo UI Mode</div>
          <div class="text-sm text-gray-300">
            WebLLM is unavailable, but you can explore the interface design.<br>
            For full functionality, try a WebGPU-compatible browser.
          </div>
        `;
        document.querySelector('.w-full.max-w-7xl').insertBefore(demoNotice, document.querySelector('.grid'));
        
        // Enable button for demo purposes (shows error when clicked)
        runButton.disabled = false;
        runButton.textContent = 'Demo Mode - Click to See Error';
        runButton.className = runButton.className.replace('bg-gray-600', 'bg-yellow-600');
      };

      // --- Initial Setup ---
      updateCodeView();
      
      // Check server configuration and initialize accordingly
      initializeApp();
    });
  </script>

</body>
</html>
